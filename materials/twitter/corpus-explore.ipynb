{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the notion of digital text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/twitter-news.json', 'r') as infile:\n",
    "    dataset = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus, corpus = [], []\n",
    "for k, v in dataset.items():\n",
    "    raw_corpus.append(v['text'])\n",
    "    m = {}\n",
    "    m['created_at'] = datetime.strftime(\n",
    "        datetime.strptime(v['created_at'],'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
    "    for z in ['favorite_count', 'hashtags', 'id', 'retweet_count', 'lang', 'text']:\n",
    "        try:\n",
    "            m[z] = v[z]\n",
    "        except KeyError:\n",
    "            m[z] = None\n",
    "    m['screen'] = v['user']['screen_name']\n",
    "    corpus.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üá∫üá∏üá®üá≥ Il supercomputer giapponese Fugaku ha toccato un picco di velocit√† pari a 415,5 petaflops, milioni di miliardi‚Ä¶ https://t.co/mgqmtteL8K\n",
      "üá∫üá∏üá®üá≥ Nel Mar cinese meridionale e intorno a Taiwan i pattugliamenti delle marine americana e cinese e delle due aer‚Ä¶ https://t.co/rxRjqmfTaH\n"
     ]
    }
   ],
   "source": [
    "for text in raw_corpus[:2]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = raw_corpus[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬´Giocava come Giggs, suonava come Bob Dylan¬ª: l‚Äôincredibile (e triste) storia di Adrian... https://t.co/133HEMDkqL https://t.co/KYGNG3GDrV\n"
     ]
    }
   ],
   "source": [
    "print(sample[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¬´Giocava', 'come', 'Giggs,', 'suonava', 'come', 'Bob', 'Dylan¬ª:', 'l‚Äôincredibile', '(e', 'triste)', 'storia', 'di', 'Adrian...', 'https://t.co/133HEMDkqL', 'https://t.co/KYGNG3GDrV']\n"
     ]
    }
   ],
   "source": [
    "print(sample[4].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¬´', 'Giocava', 'come', 'Giggs', ',', 'suonava', 'come', 'Bob', 'Dylan', '¬ª', ':', 'l', '‚Äô', 'incredibile', '(', 'e', 'triste', ')', 'storia', 'di', 'Adrian', '...', 'https://t.co/133HEMDkqL', 'https://t.co/KYGNG3GDrV']\n"
     ]
    }
   ],
   "source": [
    "tkz = TweetTokenizer()\n",
    "print(tkz.tokenize(sample[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"it_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "attributes = ['text', 'lemma_', 'pos_', 'dep_', \n",
    "              'shape_', 'is_alpha', 'is_stop']\n",
    "for token in nlp(sample[4]):\n",
    "    data = {}\n",
    "    for a in attributes:\n",
    "        data[a] = getattr(token, a)\n",
    "    words.append(data)\n",
    "S = pd.DataFrame(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | text    | lemma_   | pos_   | dep_      | shape_   | is_alpha   | is_stop   |\n",
      "|---:|:--------|:---------|:-------|:----------|:---------|:-----------|:----------|\n",
      "|  0 | ¬´       | ¬´        | PUNCT  | punct     | ¬´        | False      | False     |\n",
      "|  1 | Giocava | giocare  | VERB   | ROOT      | Xxxxx    | True       | False     |\n",
      "|  2 | come    | come     | ADP    | case      | xxxx     | True       | True      |\n",
      "|  3 | Giggs   | giggs    | PROPN  | obl       | Xxxxx    | True       | False     |\n",
      "|  4 | ,       | ,        | PUNCT  | punct     | ,        | False      | False     |\n",
      "|  5 | suonava | suonare  | VERB   | parataxis | xxxx     | True       | False     |\n",
      "|  6 | come    | come     | ADP    | case      | xxxx     | True       | True      |\n",
      "|  7 | Bob     | Bob      | PROPN  | obl       | Xxx      | True       | False     |\n",
      "|  8 | Dylan   | Dylan    | PROPN  | flat:name | Xxxxx    | True       | False     |\n",
      "|  9 | ¬ª       | ¬ª        | PUNCT  | punct     | ¬ª        | False      | False     |\n"
     ]
    }
   ],
   "source": [
    "print(S.head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "attributes = ['text', 'lemma_', 'pos_', 'dep_', \n",
    "              'shape_', 'is_alpha', 'is_stop']\n",
    "for token in nlp(sample[4].lower()):\n",
    "    data = {}\n",
    "    if token.pos_ not in ['PUNCT', 'DET'] and not token.text.startswith('http'):\n",
    "        for a in attributes:\n",
    "            data[a] = getattr(token, a)\n",
    "        words.append(data)\n",
    "S = pd.DataFrame(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['giocava' 'come' 'giggs' 'suonava' 'come' 'bob' 'dylan' 'incredibile' 'e'\n",
      " 'triste' 'storia' 'di' 'adrian']\n"
     ]
    }
   ],
   "source": [
    "print(S.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['giocare' 'come' 'giggs' 'suonare' 'come' 'bob' 'dylan' 'incredibile' 'e'\n",
      " 'triste' 'storia' 'di' 'adrian']\n"
     ]
    }
   ],
   "source": [
    "print(S.lemma_.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for i, text in enumerate(raw_corpus):\n",
    "    for token in nlp(text.lower()):\n",
    "        if token.pos_ not in ['PUNCT', 'DET'] and not token.text.startswith('http'):\n",
    "            if tf:\n",
    "                I[i][token.lemma_] += 1\n",
    "            else:\n",
    "                I[i][token.lemma_] = 1\n",
    "If = pd.DataFrame(I)\n",
    "If.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['milano', 'covid', 'come', 'cina', 'sanit√†']\n",
    "If.loc[words][[4, 24, 25, 32, 294, 31, 32, 40, 773]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_words = [x for x, y in sorted(If.sum(axis=1).items(), key=lambda x: -x[1]) if len(x) > 1][:9]\n",
    "words = ['milano', 'covid', 'come', 'cina', 'sanit√†', 'emergenza', 'ospedale', 'isolamento']\n",
    "example = If.loc[generic_words + words][[4, 24, 25, 32, 294, 31, 36, 40, 773]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfn = If[[4, 24, 25, 32, 294, 31, 36, 40, 773]].max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfnorm = 0.5 + 0.5 * example / tfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "p = sns.cubehelix_palette(8, start=2, rot=0, dark=.80, light=.95, reverse=True, as_cmap=True)\n",
    "ax = sns.heatmap(tfnorm, linewidths=.5, annot=True, cmap=p)\n",
    "ax.set_ylim([0,len(words) + len(generic_words)])\n",
    "ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize=16)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize=16, rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/alfio/Teaching/2019-20/masterdh/imgs/heatcovidtf.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [4, 24, 25, 32, 294, 31, 36, 40, 773]:\n",
    "    print('**doc {}**:'.format(d), \" \".join([x for x in raw_corpus[d].split() if not x.startswith('http')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = pd.DataFrame(np.log((If.shape[0] / If.sum(axis=1)))).loc[tfnorm.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "p = sns.cubehelix_palette(8, start=2, rot=0, dark=.80, light=.95, reverse=True, as_cmap=True)\n",
    "ax = sns.heatmap(idf, linewidths=.5, annot=True, cmap=p)\n",
    "ax.set_ylim([0,len(words) + len(generic_words)])\n",
    "ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize=16)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize=16, rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/alfio/Teaching/2019-20/masterdh/imgs/heatcovididf.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfnorm.copy()\n",
    "for k, v in idf[0].items():\n",
    "    for c, w in round(tfnorm.loc[k] * v, 2).items():\n",
    "        tfidf.loc[k][c] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "p = sns.cubehelix_palette(8, start=2, rot=0, dark=.80, light=.95, reverse=True, as_cmap=True)\n",
    "ax = sns.heatmap(tfidf, linewidths=.5, annot=True, cmap=p)\n",
    "ax.set_ylim([0,len(words) + len(generic_words)])\n",
    "ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize=16)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize=16, rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/alfio/Teaching/2019-20/masterdh/imgs/heatcovidtfidf.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
